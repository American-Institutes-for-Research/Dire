<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Dire">
<title>Marginal Maximum Likelihood Regression Estimation • Dire</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.0/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.0/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Marginal Maximum Likelihood Regression Estimation">
<meta property="og:description" content="Dire">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">Dire</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.0.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/MML.html">Marginal Maximum Likelihood Regression Estimation</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/American-Institutes-for-Research/Dire/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">



<script src="MML_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <main id="main"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Marginal Maximum Likelihood Regression Estimation</h1>
                        <h4 data-toc-skip class="author">Developed by Paul Bailey and Harold Doran</h4>
            
            <h4 data-toc-skip class="date">March 27, 2020</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/American-Institutes-for-Research/Dire/blob/HEAD/vignettes/MML.Rmd" class="external-link"><code>vignettes/MML.Rmd</code></a></small>
      <div class="d-none name"><code>MML.Rmd</code></div>
    </div>

    
    
<p>This document describes Marginal Maximum Likelihood (MML) estimation for student test data in the <code>Dire</code> package. In these models, failing to account for the measurement variance can bias the regression or variance estimates.</p>
<p>The student test data are assumed to have been generated by an Item Response Theory (IRT) model, where students’ responses are correct or incorrect (have an increasing score) when student <span class="math inline">\(i\)</span> has higher ability (<span class="math inline">\(\theta_i\)</span>) and decreasing when the item is more difficult (<span class="math inline">\(d_j\)</span>), so the probability of a correct response increases as the quantity <span class="math inline">\(\theta_i - d_j\)</span> increases. See the appendix for the likelihood functions for various response models.</p>
<p>This package considers a regression model of the following form (one case in Cohen &amp; Jiang 1999): <span class="math display">\[\begin{align}
\bm{\theta} = \bm{X\beta} + \bm{\epsilon}
\end{align}\]</span> where <span class="math inline">\(\bm{\theta}\)</span> is a vector of student abilities, <span class="math inline">\(\bm{X}\)</span> is a matrix of covariates with unknown parameters <span class="math inline">\(\bm{\beta}\)</span> and residual variance <span class="math inline">\(\bm{\epsilon}\)</span>. For estimation, we assume that the residual variance is normally distributed without covariance across observations (students) sharing variance of unknown level <span class="math inline">\(\sigma^2\)</span> so that <span class="math display">\[\begin{align}
\bm{\epsilon} \sim N(\bm{0},\sigma^2 \bm{I})
\end{align}\]</span> where <span class="math inline">\(N(\bm{0}, \sigma^2 \bm{I})\)</span> is the normal distribution with mean zero, and covariance <span class="math inline">\(\sigma^2 \bm{I}\)</span>, and <span class="math inline">\(\bm{I}\)</span> is the identity matrix. The variance estimation then allows for covariances between students (e.g., in a two-stage sample or clustered within schools).</p>
<p>The next section describes the estimation of <span class="math inline">\(\bm{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>. The final section describes five methods for variance estimation available in MML estimation, including the traditional (consistent) method, two heteroskedasticity robust methods, and two methods appropriate to a two-stage survey sample, such as the National Assessment of Educational Progress (NAEP).</p>
<div class="section level2">
<h2 id="parameter-estimation">Parameter Estimation<a class="anchor" aria-label="anchor" href="#parameter-estimation"></a>
</h2>
<p>Student test data consist of a series of items on which a student receives a score. The matrix <span class="math inline">\(\bm{R}\)</span> has row <span class="math inline">\(i\)</span> regarding a student and column <span class="math inline">\(j\)</span> regarding an item so that <span class="math inline">\(R_{ij}\)</span> is student <span class="math inline">\(i\)</span>’s score on item <span class="math inline">\(j\)</span> and takes on integer values from 0 to the maximum score on the item. Many possible models exist for the <span class="math inline">\(\bm{R}\)</span> matrix data, which are covered, briefly, in the appendix to this document. The rest of this document simply assumes that item parameters have been estimated with a consistent estimator and are treated as being estimated without error.</p>
<p>In an MML model for test data for <span class="math inline">\(N\)</span> individuals, conditional on a set of parameters for a set of <span class="math inline">\(K\)</span> test items, the likelihood of a regression equation is <span class="math display">\[\begin{align}
\mathcal{L} (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) = \prod_{i=1}^N  \left[ \int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} \exp \frac{-(\theta_i - \bm{X}_i \beta)^2}{2\sigma^2}  \, \prod_{j=1}^K {\rm Pr}(\bm{R}_{ij}|\theta_i,\bm{P}_j) d\theta_i \right]^{\bm{w}_i}
\end{align}\]</span> where <span class="math inline">\(\mathcal{L}\)</span> is the likelihood of the regression parameters <span class="math inline">\(\bm{\beta}\)</span> with full sample weights <span class="math inline">\(\bm{w}_i\)</span> conditional on item score matrix <span class="math inline">\(\bm{R}\)</span><em>,</em> student covariate matrix <span class="math inline">\(\bm{X}\)</span><em>,</em> and item parameter data <span class="math inline">\(\bm{P}\)</span><em>;</em> <span class="math inline">\(\sigma^2\)</span> is the variance of the regression residual; <span class="math inline">\(\theta_i\)</span> is the <span class="math inline">\(i\)</span>th student’s latent ability measure that is being integrated out; <span class="math inline">\({\rm Pr}(\bm{R}_{ij}|\theta_i, \bm{P}_j)\)</span> is the probability of individual <span class="math inline">\(i\)</span>’s score on test item <span class="math inline">\(j\)</span><em>,</em> conditional on the student’s ability and item parameters <span class="math inline">\(\bm{P}_j\)</span>—see the appendix for example forms of <span class="math inline">\({\rm Pr}(\bm{R}_{ij}|\theta_i, \bm{P}_j)\)</span>. Note that if the user is only interested in the population mean, it can be regarded as a special case; <span class="math inline">\(\bm{X}\)</span> is a vector of all ones, and the value of <span class="math inline">\(\bm{\beta}\)</span> has only one element that is the mean estimate.</p>
<p>The integral is evaluated using the trapezoid rule at quadrature points <span class="math inline">\(t_q\)</span> and quadrature weights <span class="math inline">\(\delta\)</span> so that <span class="math display">\[\begin{align}
\mathcal{L} (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) &amp;= \prod_{i=1}^N \left[ \sum_{q=1}^Q \delta \frac{1}{\sigma \sqrt{2\pi}} \exp \frac{-(t_q - \bm{X}_i \bm{\beta})^2}{2\sigma^2} \prod_{j=1}^K  {\rm Pr}(\bm{R}_{ij}|t_q, \bm{P}_j)\right]^{\bm{w}_i}
\end{align}\]</span> where <span class="math inline">\(\delta\)</span> is the distance between any two uniformly spaced quadrature points so that <span class="math inline">\(\delta = t_{q+1} - t_{q}\)</span> for any <span class="math inline">\(q\)</span> that is at least one and less than <span class="math inline">\(Q\)</span>. The range and value of <span class="math inline">\(Q\)</span> parameterize the quadrature, and its accuracy and should be varied to ensure convergence. The advantage of the trapezoidal rule is that the fixed quadrature points allow the values of the probability to be calculated once per student.</p>
<p>The variance formulas use the log-likelihood, which is given by <span class="math display">\[\begin{align}
\ell (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) &amp;= \sum_{i=1}^N \bm{w}_i \, {\rm log} \left[\delta \sum_{q=1}^Q \frac{1}{\sigma \sqrt{2\pi}} \exp \frac{-(t_q - \bm{X}_i \bm{\beta})^2}{2\sigma^2} \prod_{j=1}^K  {\rm Pr}(\bm{R}_{ij}|t_q, \bm{P}_j) \right]
\end{align}\]</span> Note that <span class="math inline">\(\delta\)</span> can be removed for optimization, and its presence adds <span class="math inline">\({\rm log}(\delta) \sum \bm{w}_i\)</span> to the log-likelihood.</p>
<div class="section level3">
<h3 id="composite-scores">Composite Scores<a class="anchor" aria-label="anchor" href="#composite-scores"></a>
</h3>
<p>When the outcome of interest is composite scores, the parameters are estimated by separately estimating the coefficients for each subscale (<span class="math inline">\(\bm{\beta}_s\)</span> for subscale <span class="math inline">\(s\)</span>) and then calculating the composite scores (<span class="math inline">\(\bm{\beta}_c\)</span>) using subscale weights (<span class="math inline">\(\omega_s\)</span>). <span class="math display">\[\begin{align}
\bm{\beta}_c &amp;= \sum_{s=1}^S \omega_s \bm{\beta}_s \label{eq:composite}
\end{align}\]</span> where there are <span class="math inline">\(S\)</span> subscales.</p>
<p>For variance estimation, the covariance matrix (<span class="math inline">\(\bm{\Sigma}\)</span>) between subscales is of interest. The covariance terms are estimated one at a time using the submatrix <span class="math display">\[\begin{align}
\bm{\Sigma}_{ij} = \left[ \begin{array}{cc} s_i &amp; s_{ij} \\ s_{ij} &amp; s_j \end{array} \right]
\end{align}\]</span></p>
<p>so that the two are jointly bivariate normally distributed <span class="math display">\[\begin{align}
\left( \begin{array}{c} \beta_i \\ \beta_j \end{array} \right) | \bm{\Sigma}_{ij}, \bm{w}, \bm{R}, \bm{X}, \bm{P} &amp; \sim {\rm MVN} \left( \left. \left( \begin{array}{c} \beta_i \\ \beta_j \end{array} \right), \bm{\Sigma}_{ij} \right| \bm{w}, \bm{R}, \bm{X}, \bm{P} \right) \label{eq:compcov}
\end{align}\]</span> where <span class="math inline">\({\rm MVN}(u, S|\cdot)\)</span> is the multivariate normal density function with mean <span class="math inline">\(u\)</span> and covariance <span class="math inline">\(S\)</span>, conditional on <span class="math inline">\(\cdot\)</span>, which are additional parameters.</p>
<p>The likelihood is then <span class="math display">\[\begin{align}
\ell \left( s_{ij} \left| \beta_i, \beta_j , s_i, s_j; \bm{w}, \bm{R}, \bm{X}, \bm{P}\right. \right) &amp;=
\sum_{n=1}^N \bm{w}_n \, {\rm log} \left\{ \delta^2 \sum_{q_i=1}^Q \sum_{q_j=1}^Q \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{|\bm{\Sigma}|}}\exp \left( \bm{r}_{q_1q_2}^T \bm{\Sigma}^{-1} \bm{r}_{q_1 q_2} \right) \right. \\
&amp;\mathrel{\phantom{=}} \left. \times \left[ \prod_{k=1}^K  {\rm Pr}(\bm{R}_{nk}|t_{q_1}, \bm{P}_k) \right] \left[ \prod_{k=1}^K {\rm Pr}(\bm{R}_{nk}|t_{q_2}, \bm{P}_k) \right] \right\}
\label{eq:compcovlnl}
\end{align}\]</span> where <span class="math inline">\(|\bm{\Sigma}|\)</span> is the determinant of <span class="math inline">\(\bm{\Sigma}\)</span>, and the residual term is defined as <span class="math display">\[\begin{align}
\bm{r}_{q_1 q_2} = \left( \begin{array}{c} t_{q_1} - \bm{X}_n \bm{\beta}_i  \\ t_{q_2} - \bm{X}_n \bm{\beta}_j \end{array} \right) 
\end{align}\]</span> Notice that the parameters <span class="math inline">\(\bm{\beta}_i\)</span>, <span class="math inline">\(\bm{\beta}_j\)</span>, <span class="math inline">\(s_i\)</span>, and <span class="math inline">\(s_j\)</span> are used from the by-subscale estimation and optimization of the density function is exclusively over the covariance term <span class="math inline">\(s_{ij}\)</span>.</p>
<p>The joint distribution of the vector <span class="math display">\[\begin{align}
\bm{\beta}_{\cdot} = \left( \begin{array}{c} \bm{\beta}_1 \\ \vdots \\ \bm{\beta}_S \end{array} \right)
\end{align}\]</span> is then <span class="math display">\[\begin{align}
\bm{\beta}_{\cdot} ,\bm{\Sigma} | \bm{w}, \bm{R}, \bm{X}, \bm{P} &amp;= {\rm MVN}(\left. \bm{\beta}_{\cdot}, \bm{\Sigma} \right| \bm{w}, \bm{R}, \bm{X}, \bm{P})   \label{eq:lnlcomposite}
\end{align}\]</span> which has an intractably high dimensional log-likelihood because it involves <span class="math inline">\(S\)</span> sums inside the log-likelihood.</p>
</div>
</div>
<div class="section level2">
<h2 id="variance-estimation">Variance Estimation<a class="anchor" aria-label="anchor" href="#variance-estimation"></a>
</h2>
<p>Estimating variance of the parameters <span class="math inline">\(\bm{\beta}\)</span> can be done in one of several ways.</p>
<p>The inverse Hessian matrix is a consistent estimator when the estimator of <span class="math inline">\(\bm{\beta}\)</span> is consistent (Green, 2003, p. 520): <span class="math display">\[\begin{align}
{\rm Var}(\bm{\beta}) = -\bm{H}(\bm{\beta})^{-1} =  - \left[\frac{\partial^2 \ell(\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X})}{\partial \bm{\beta}^2} \right]^{-1} \label{eq:vbeta}
\end{align}\]</span> This variance is returned when the variance method is set to <code>consistent</code> or left as the default.</p>
<p>A class of variance estimators typically called “sandwich” or “robust” variance estimators allow for variation in the residual and are of the form <span class="math display">\[\begin{align}
{\rm Var}(\bm{\beta}) = H(\bm{\beta})^{-1} \bm{V} H(\bm{\beta})^{-1} \label{eq:sandwich}
\end{align}\]</span> where <span class="math inline">\(V\)</span> is an estimate of the variance of the summed score function (Binder, 1983).</p>
<p>For a convenience sample, we provide two robust estimators. First, the so-called <code>robust</code> (Huber or Huber-White) variance estimator uses <span class="math display">\[\begin{align}
\bm{V} &amp;= \sum_{i=1}^N \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta} \right] \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta} \right]^{'}
\end{align}\]</span></p>
<p>Second, for the <code>cluster robust</code> case, the partial derivatives are summed within the cluster so that <span class="math display">\[\begin{align}
\bm{V} &amp;= \sum_{c=1}^{n^\prime} \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} \right] \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} \right]^{'}
\end{align}\]</span> where there are <span class="math inline">\(n^\prime\)</span> clusters, indexed by <span class="math inline">\(c\)</span>, and the partial derivatives are summed within the group of which there are <span class="math inline">\(n_c\)</span> members: <span class="math display">\[\begin{align}
\frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} &amp;= \sum_{i=1}^{n_c} \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta}
\end{align}\]</span></p>
<p>We also provide two survey sampling variance estimation techniques. The first one uses replicate weights, either from the jackknife, including Fay’s method for the jackknife, or from balanced repeated replication. In this approach, the typical method of estimating sampling variance still works, and the sampling covariance matrix can be calculated as <span class="math display">\[\begin{align}
{\rm Var}(\bm{\beta}) &amp;= \sum_{j=1}^J \left(\bm{\beta}_j - \bm{\beta}_0 \right) \left(\bm{\beta}_j - \bm{\beta}_0 \right)^{'}
\end{align}\]</span> where there are <span class="math inline">\(J\)</span> replicate weights and the result of applying direct estimation under the set of weights <span class="math inline">\(j\)</span> is <span class="math inline">\(\bm{\beta}_j\)</span>, whereas <span class="math inline">\(\bm{\beta}_0\)</span> is the estimate of <span class="math inline">\(\bm{\beta}\)</span> under the full sample weights. We recomend using this method when <code>replicate</code> variance estimation is requested.</p>
<p>The second survey sampling method is called the <code>Taylor series</code> method and uses the same formula as Eq. , but <span class="math inline">\(\bm{V}\)</span> is the estimate of the variance of the score vector (Binder, 1983). Our implementation assumes a two-stage design with <span class="math inline">\(n_a\)</span> primary sampling units (PSUs) in stratum <span class="math inline">\(a\)</span> and summed across the <span class="math inline">\(A\)</span> strata according to <span class="math display">\[\begin{align}
\bm{V} &amp;= \sum_{a=1}^A \bm{V}_a
\end{align}\]</span> where <span class="math inline">\(\bm{V}_a\)</span> is a variance estimate for stratum <span class="math inline">\(a\)</span> and is defined by <span class="math display">\[\begin{align}
\bm{V}_a &amp;= \frac{n_a}{n_a -1} \sum_{p=1}^{n_a} \left( \bm{s}_p - \bar{\bm{s}}_a \right)\left( \bm{s}_p - \bar{\bm{s}}_a \right)' \label{eq:Va}
\end{align}\]</span> where <span class="math inline">\(s_p\)</span> is the sum of the weighted (or pseudo-) score vector that includes all units in PSU <span class="math inline">\(p\)</span> in stratum <span class="math inline">\(a\)</span> and <span class="math inline">\(\bar{\bm{s}}_a\)</span> is the (unweighted) mean of the <span class="math inline">\(\bm{s}_p\)</span> terms in stratum <span class="math inline">\(a\)</span> so that <span class="math display">\[\begin{align}
s_p &amp;=\sum_{i \in {\rm PSU} \ p}\frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta}    &amp; \bar{\bm{s}}_a&amp;= \frac{1}{n_a} \sum_{p \in {\rm stratum} \ a} s_p
\end{align}\]</span></p>
<p>When a stratum has only one PSU, <span class="math inline">\(\bm{V}_a\)</span> is undefined. The best approach is for the analyst to adjust the strata and PSU identifiers, in a manner consistent with the sampling approach, to avoid singleton strata. Two simpler but less defensible options are available. First, the strata with single PSUs can be dropped from the variance estimation, yielding an underestimate of the variance.</p>
<p>The second option is for the singleton stratum to use the overall mean of <span class="math inline">\(s_p\)</span> in place of <span class="math inline">\(\bar{s}_a\)</span>. So, <span class="math display">\[\begin{align}
\bar{\bm{s}} &amp;= \frac{1}{n^\prime} \sum s_p
\end{align}\]</span> where the sum is across all PSUs, and <span class="math inline">\(n^\prime\)</span> is the number of PSUs across all strata. Then, for each singleton stratum, Eq.  becomes <span class="math display">\[\begin{align}
\bm{V}_a &amp;= 2 \left( \bm{s}_p - \bar{\bm{s}} \right)\left( \bm{s}_p - \bar{\bm{s}} \right)' \label{eq:Va1}
\end{align}\]</span> where the value 2 is used in place of <span class="math inline">\(\frac{n_a}{n_a-1}\)</span>, which is undefined when <span class="math inline">\(n_a=1\)</span>. This option can underestimate the variance but is thought to more likely overestimate it.</p>
<div class="section level3">
<h3 id="composite-scores-1">Composite Scores<a class="anchor" aria-label="anchor" href="#composite-scores-1"></a>
</h3>
<p>The likelihood of composite scores (Eq. ) is additively separable, the covariances (including the variances) can be calculated in two steps using Eq. . First, the covariance matrix of <span class="math inline">\(\bm{\xi}\)</span> is formed, and then the composite covariance terms are estimated as the variance of a linear combination of the elements of <span class="math inline">\(\bm{\xi}\)</span>.</p>
<p>In the first step, any of the methods in the section “Variance Estimation” are applied to Eq. , treating <span class="math inline">\(\bm{\xi}\)</span> in the same fashion Eq.  treats <span class="math inline">\(\bm{\beta}\)</span>. This step results in a block diagonal inverse Hessian matrix, with a block for each subscale, and a potentially dense matrix for <span class="math inline">\(\bm{V}\)</span>. Each matrix is square and has <span class="math inline">\(S \cdot (\zeta+1)\)</span> rows and columns, where <span class="math inline">\(\zeta\)</span> is the number of elements in the regression formula (each subscale), to which one is added for the <span class="math inline">\(\sigma\)</span> terms.</p>
<p>This step results in the following matrix: <span class="math display">\[\begin{align}
{\rm Var}(\bm{\xi})=H(\bm{\xi})^{-1} \bm{V} H(\bm{\xi})^{-1}
\end{align}\]</span></p>
<p>For the second step, the composite coefficient then has an <span class="math inline">\(i\)</span>th variance term of <span class="math display">\[\begin{align}
{\rm Var}(\bm{\xi}_{ci}) &amp;= \bm{e}_i H(\bm{\xi})^{-1} \bm{V} H(\bm{\xi})^{-1} \bm{e}_i
\end{align}\]</span> where <span class="math inline">\(\bm{\xi}_{ci}\)</span> is the composite coefficient for the <span class="math inline">\(i\)</span>th coefficient, and <span class="math inline">\(\bm{e}_{i}\)</span> is the vector of weights arranged such that <span class="math display">\[\begin{align}
\bm{\xi}_{ci} = \bm{e}_{i}^T \bm{\xi}
\end{align}\]</span> The covariance between two terms, <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span><em>,</em> is a simple extension <span class="math display">\[\begin{align}
{\rm Cov}(\bm{\beta}_{ci}, \bm{\beta}_{cj}) &amp;= \bm{e}_i H(\bm{\beta})^{-1} \bm{V} H(\bm{\beta})^{-1} \bm{e}_j
\end{align}\]</span> which uses the definition, <span class="math display">\[\begin{align}
\bm{\xi}_{cj} &amp;= \bm{e}_{j}^T \bm{\xi}
\end{align}\]</span></p>
<p>A simple example may help clarify. Imagine a composite score composed of two subscales, 1 and 2, with weights <span class="math inline">\(\omega_1 = 0.4\)</span> and <span class="math inline">\(\omega_2=0.6\)</span>. Supposed a user is interested in a regression of the form <span class="math display">\[\begin{align}
\theta = a + x_1 \cdot b + \epsilon \label{eq:compex} \\
\epsilon \sim N(0,\sigma)
\end{align}\]</span> Then the regression in Eq.  would be fit once for subscale 1 and once for subscale 2; the first fit would yield estimated values <span class="math inline">\(\left\{ a_1, b_1, \sigma_1 \right\}\)</span>, and the second fit would yield <span class="math inline">\(\left\{ a_2, b_2, \sigma_2 \right\}\)</span>. The estimated value, for example, <span class="math inline">\(a_c\)</span>, would be <span class="math inline">\(a_c = 0.4\cdot\alpha_1 + 0.6\cdot\alpha_2\)</span>. By stacking the estimates together, <span class="math display">\[\begin{align}
\bm{\theta} &amp;= \begin{bmatrix}
 a_1 \\
 b_1 \\
 \sigma_1\\
 a_2 \\
 b_2\\
 \sigma_2
\end{bmatrix}
\end{align}\]</span> the covariance matrix can then be estimated and will result in a matrix <span class="math inline">\(\bm{\Omega} \equiv {\rm Var}(\bm{\beta})\)</span> from Eq.  that has six rows and six columns. Using the vector <span class="math display">\[\begin{align}
\bm{e}_1 &amp;= \begin{bmatrix}
 0.4 \\
 0 \\
 0\\
 0.6 \\
 0\\
 0
\end{bmatrix}
\end{align}\]</span> it can easily be confirmed that <span class="math inline">\(a_c = \bm{e}_1^T \bm{\xi}\)</span>, so <span class="math inline">\({\rm Var}(a_c)= \bm{e}_1^T \bm{\Omega} \bm{e}_1\)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Binder, D. A. (1983). On the variances of asymptotically normal estimators from complex surveys. <em>International Statistical Review</em>, <em>51</em>(3), 279–292.</p>
<p>Black, P. E. (2019). Big-O notation. In P. E. Black (Ed.), <em>Dictionary of algorithms and data structures</em>. Washington, DC: National Institute of Standards and Technology. Retrieved from <a href="https://www.nist.gov/dads/HTML/bigOnotation.html" class="external-link uri">https://www.nist.gov/dads/HTML/bigOnotation.html</a></p>
<p>Cohen, J. D., &amp; Jiang, T. (1999). Comparison of partially measured latent traits across nominal subgroups. <em>Journal of the American Statistical Association</em>, 94(<em>448</em>), 1035–1044.</p>
<p>Green, W. H. (2003). <em>Econometric analysis</em> Upper Saddle River, NJ: Prentice Hall.</p>
<p>Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. <em>Proceedings of the Fifth Berkeley Symposium of Mathematical Statistics and Probability</em>, Vol. I: <em>Statistics</em> (pp. 221–233). Berkeley, CA: University of California Press.</p>
<p>Johnson, S. G. (2010). <em>Notes on the convergence of trapezoidal-rule quadrature</em>. Retrieved from <a href="https://math.mit.edu/~stevenj/trapezoidal.pdf" class="external-link uri">https://math.mit.edu/~stevenj/trapezoidal.pdf</a></p>
<p>McCullagh, P. &amp; Nelder, J. A. (1989). <em>Generalized linear models</em>. (2nd ed.). London, UK: Chapman &amp; Hall/CRC.</p>
<p>NAEP. (2008). The generalized partial credit model [NAEP Technical Documentation Website]. Retrieved from <a href="https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_gen.aspx" class="external-link uri">https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_gen.aspx</a>.</p>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <em>Econometrica</em>, 48(<em>4</em>), 817–838.</p>
</div>
<div class="section level2">
<h2 id="appendix--test-probability-density-functions">Appendix. Test Probability Density Functions<a class="anchor" aria-label="anchor" href="#appendix--test-probability-density-functions"></a>
</h2>
<p>For all cases scored as either correct or incorrect, we use the <em>three parameter logit</em> (3PL) model: <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= g_j + \frac{1-g_j}{1+\exp\left[ -D \, a_j \, (\theta_i - d_j)\right]} \label{eq:3PL}
\end{align}\]</span> where <span class="math inline">\(g_j\)</span> is the guessing parameter, <span class="math inline">\(a_j\)</span> is the discrimination factor, <span class="math inline">\(d_j\)</span> is the item difficulty, and <span class="math inline">\(D\)</span> is a constant, usually set to 1.7, to map the <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(d_j\)</span> terms to a probit-like space; this term is applied by tradition.</p>
<p>When a <em>two parameter logit</em> (2PL) is used, Eq.  is modified to omit <span class="math inline">\(g_j\)</span> (effectively setting it to zero): <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[ -D \, a_j \, (\theta_i - d_j)\right]} \label{eq:2PL}
\end{align}\]</span></p>
<p>When a <em>Rasch model</em> is used, Eq.  is further modified to set all <span class="math inline">\(a_j\)</span> to a single <span class="math inline">\(a\)</span><em>,</em> and <span class="math inline">\(D\)</span> is set to one. <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[ - a \, (\theta_i - d_j)\right]} \label{eq:Rasch}
\end{align}\]</span></p>
<p>The <em>Graded Response Model</em> (GRM) has a probability density that generalizes an ordered logit (McCullagh &amp; Nelder, 1989): <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[-D\, a_j \, (\theta_i - d_{R_{ij},j})\right]} - \frac{1}{1+\exp\left[-D\, a_j \, (\theta_i - d_{1+R_{ij},j})\right]} \label{eq:grm}
\end{align}\]</span> Here the parameters <span class="math inline">\(\bm{P}_j\)</span> are the cut points <span class="math inline">\(d_{cj}\)</span>, where <span class="math inline">\(d_{0j}=-\infty\)</span> and <span class="math inline">\(d_{C+1,j}=\infty\)</span>. In the first term on the right side of Eq. , the subscript <span class="math inline">\(R_{ij}\)</span> on <span class="math inline">\(d_{R_{ij},j}\)</span> indicates it is the cut point associated with the response level to item <span class="math inline">\(j\)</span> for person <span class="math inline">\(i\)</span>, whereas the last subscript (<span class="math inline">\(j\)</span>) indicates that it is the <span class="math inline">\(d\)</span> term for item <span class="math inline">\(j\)</span>. In the second term, the cut point above that cut point is used.</p>
<p>The <em>Generalized Partial Credit Model</em> (GPCM) has a probability density that generalizes a multinomial logit (McCullagh &amp; Nelder, 1989) <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{\exp \left[ \sum_{c=0}^{R_{ij}} D a_j (\theta_i - d_{cj}) \right]}{\sum_{r=0}^{C} \exp \left[ \sum_{c=0}^{r} D a_j (\theta_i - d_{cj}) \right]}
\end{align}\]</span> where <span class="math inline">\(c\)</span> indexes cut points, of which there are <span class="math inline">\(C\)</span><em>,</em> and <span class="math inline">\(j\)</span> indexes the item.</p>
<p>The GPCM equation has an indeterminancy because all <span class="math inline">\(d_j\)</span> terms could increase and make the values of the probability the same. We can solve the indeterminacy in several ways.</p>
<p>NAEP (2008) uses a mean difficulty (<span class="math inline">\(b_j\)</span>), and the <span class="math inline">\(d_j\)</span> values are then given by <span class="math display">\[\begin{align}
d_{0j} &amp;= 0 &amp;       d_{cj} &amp;= b_j - \delta_{jc} \, ; \, 1 \leq c \leq C
\end{align}\]</span> where the <span class="math inline">\(\delta_{jc}\)</span> values are estimated so that <span class="math inline">\(0=\sum_{c=1}^{C} \delta_{jc}\)</span>. In this package, when the <code>polyParamTab</code> has an <code>itemLocation</code>, it serves as <code>b</code>. When there is no <code>itemLocation</code>, the package uses the <span class="math inline">\(\delta\)</span> values directly <span class="math display">\[\begin{align}
d_{0j} &amp;= 0 &amp;       d_{cj} &amp;= \delta_{jc} \, ; \, 1 \leq c \leq C
\end{align}\]</span></p>
<p>When a <em>Partial Credit Model</em> (PCM) is used, and the value of <span class="math inline">\(D\)</span> is set to one, whereas <span class="math inline">\(a_j\)</span> is again shared across all items. So <span class="math display">\[\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{\exp \left[ \sum_{c=0}^{R_{ij}} a (\theta_i - d_{cj}) \right]}{\sum_{r=0}^{C} \exp \left[ \sum_{c=0}^{r} a (\theta_i - d_{cj}) \right]}
\end{align}\]</span></p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Paul Bailey, Eric Buehler, Sun-joo Lee, Harold Doran.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
