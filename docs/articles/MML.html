<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Weighted Marginal Maximum Likelihood Regression Estimation • Dire</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Weighted Marginal Maximum Likelihood Regression Estimation">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">Dire</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">3.0.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/MML.html">Weighted Marginal Maximum Likelihood Regression Estimation</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/American-Institutes-for-Research/Dire/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Weighted Marginal Maximum Likelihood Regression Estimation</h1>
                        <h4 data-toc-skip class="author">Developed by
Paul Bailey and Harold Doran</h4>
            
            <h4 data-toc-skip class="date">March 27, 2020</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/American-Institutes-for-Research/Dire/blob/HEAD/vignettes/MML.Rmd" class="external-link"><code>vignettes/MML.Rmd</code></a></small>
      <div class="d-none name"><code>MML.Rmd</code></div>
    </div>

    
    
<p>This document describes weighted Marginal Maximum Likelihood (MML)
estimation for student test data in the <code>Dire</code> package. The
model treats abilities as a latent variable and estimates a linear model
in the latent space. In this framework, failing to account for the
measurement variance will bias the regression estimates. The
<code>Dire</code> package can directly estimate these models in the
latent space. Aditionally, it can generate palausible values (PVs) which
allow for unbiased estimation using the same formulas as multiple
imputation (Mislevy et al.).</p>
<p>For simplicity, focusing first on a univariate model where there is
only one latent ability measured, the student test data are assumed to
have been generated by an Item Response Theory (IRT) model, where
student
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
has ability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_{i}</annotation></semantics></math>.
The probability of a student getting an item correct—for a dichotomous
item—increases in ability but decreases in item difficulty. Put
together, the likelihood of student
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>’s
responss to items
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mi>i</mi></msub><annotation encoding="application/x-tex">R_{i}</annotation></semantics></math>)
is a function of the student’s latent ability in the construct
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_{i}</annotation></semantics></math>),
the item parameters of each
item(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
<span class="math display">$$\begin{align}
\mathcal{L}(\bm{R}_{i}| \theta_{i} , \bm{P} )= \prod_{h=1}^{H} {\rm
Pr}(R_{ih}| \theta_{i}, \bm{P}_h)
\end{align}$$</span> where the probability function in the product is
showin in the appendix and depends on the IRT model used for the item.
it is possible that a student will not have been shown an item and then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">R_{ij}</annotation></semantics></math>
is missing. The missing response does not change the probability. This
can be thought of as <span class="math inline">${\rm Pr}(R_{ih}|
\theta_{i}, \bm{P}_h)$</span> being 1 for all values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_{i}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐏</mi><mi>h</mi></msub><annotation encoding="application/x-tex">\bm{P}_h</annotation></semantics></math>.</p>
<p>This latent trait is then modeled as a function of a row vector of
explanatory variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{X}_i</annotation></semantics></math>
so that.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><msub><mi>𝐗</mi><mi>i</mi></msub><mi>𝛃</mi><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\theta_i = \bm{X}_i \bm{\beta} + \epsilon_i
\end{align}</annotation></semantics></math> Where
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\epsilon_i \sim N(0,\sigma)
\end{align}</annotation></semantics></math></p>
<p>This creates two sets of likelihoods for person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
one associated with the covariates
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{X}_i</annotation></semantics></math>)
and another associated with the observed item responses
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐑</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{R}_i</annotation></semantics></math>).
The likelihood for the student is the product of these two. <span class="math display">$$\begin{align}
\mathcal{L}(\bm{R}_{i}| \bm{\beta}, \sigma, \bm{X}_i , \bm{P} ) = \int
f(\theta_i) \cdot \prod_{h=1}^{H} {\rm Pr}(R_{ih}| \theta_i, \bm{P}_h)
d\theta_i
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\theta_i)</annotation></semantics></math>
is the density of student
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>s
latent ability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math>.
Substituting the formula for above, <span class="math display">$$\begin{align}
\mathcal{L}(\bm{R}_{i}| \bm{\beta}, \sigma, \bm{X}_i , \bm{P} ) = \int
\frac{1}{\sigma \sqrt{2\pi}}\exp(\frac{-\epsilon_i^2}{2\sigma})
\prod_{h=1}^{H} {\rm Pr}(R_{ih}| \bm{X}_i \beta + \epsilon_i, \bm{P}_h)
d\epsilon_i
\end{align}$$</span> The term in the integral is the convolusion of two
functions and there is no closed form solution. <code>Dire</code>
follows the AM statistical software (Cohen &amp; Jiang 1999) and uses
fixed quadrature points set by the user.</p>
<p>In addition, Dire allows for the construct being scored to not be a
simple univariate construct but to incorporate several correlated
subscales or constructs. If these are labeled from one to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>
then each student has a vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛉</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\theta}_i</annotation></semantics></math>
with components
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\theta_{ij}</annotation></semantics></math>.
This leads to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>
latent regression equations and requires a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>-demensional
integral. <span class="math display">$$\begin{align}
\mathcal{L} (\bm{R}_{i}| \bm{\beta}_1, ..., \bm{\beta}_J, \bm{\Sigma},
\bm{X}_i , \bm{P} ) = \int ... \int
\frac{1}{(2\pi)^{\frac{J}{2}}\sqrt{{\rm det}(\bm{\Sigma})}}
\exp(-\frac{1}{2}\bm{\epsilon}_i^T \bm{\Sigma} \bm{\epsilon}_i)
\prod_{j=1}^J \prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| \theta_{ij},
\bm{P}_{hj}) d\theta_{i1} ... d\theta_{iJ} \label{eq:introeq}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">\bm{R}_{ijh}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐏</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\bm{P}_{hj}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{\beta}_j</annotation></semantics></math>
all have a subscript
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
added because there is now a set of them for each subscale
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛜</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\epsilon}_i</annotation></semantics></math>
is now a vector with elements
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ϵ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\epsilon_{ij}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation></semantics></math>
is the covariance matrix for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\bm{\epsilon}</annotation></semantics></math>
vector, which allows for covariance between constructs at the student
level in that when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Σ</mi><mrow><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup></mrow></msub><annotation encoding="application/x-tex">\Sigma_{jj^\prime}</annotation></semantics></math>
is positive than a student with a higher score on construct
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>,
conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐗</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{X}_i</annotation></semantics></math>
will also be expected to have a higher score on construct
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>j</mi><mi>′</mi></msup><annotation encoding="application/x-tex">j^\prime</annotation></semantics></math>.</p>
<p>This problem suffers from the curse of dimensionality. However, as is
pointed out by Cohen and Jiang (1999) this is identical to seemingly
unrelated regression (SUR) and can be solved by fitting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\sigma_j</annotation></semantics></math>
once for each subscale and then identifying each of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>J</mi><mn>2</mn></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">{ J \choose 2}</annotation></semantics></math>
covariance terms in a pairwise fashion.</p>
<p>This comes from the fact that the multivariate normal can be
decomposed into two multivariate normals, one conditional on the other.
First re-writing, eq with the normal rewritten as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\epsilon_i)</annotation></semantics></math><span class="math display">$$\begin{align}
\mathcal{L} (\bm{R}_{i}| \bm{\beta}_1, ..., \bm{\beta}_J, \bm{\Sigma},
\bm{X}_i , \bm{P} ) =  \int ... \int  f(\epsilon_i) \prod_{j=1}^J
\prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| \theta_{ij}, \bm{P}_{hj})
d\theta_{i1} ... d\theta_{iJ} \label{eq:introeq}
\end{align}$$</span> the multivariate normal can be decomposed into the
product of two distributions, an unconditional (univariate-)normal and a
condional (potentially multivariate when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">J&gt;2</annotation></semantics></math>)
normal. Using the first integration dimension, without loss of
generality, this becomes <span class="math display">$$\begin{align}
\mathcal{L} (\bm{R}_{i}| \bm{\beta}_1, ..., \bm{\beta}_J, \bm{\Sigma},
\bm{X}_i , \bm{P} ) =  \int ... \int  f_1(\epsilon_{i1})
f_{-1}(\epsilon_{i,-1}|\epsilon_{i1}) \prod_{j=1}^J \prod_{h_j=1}^{H_j}
{\rm Pr}(R_{ijh}| \theta_{ij}, \bm{P}_{hj}) d\theta_{i1} ...
d\theta_{iJ} \label{eq:introeq}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ϵ</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_1(\epsilon_{i1})</annotation></semantics></math>
is the normal density function for a single variable and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>−</mi><mn>1</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ϵ</mi><mrow><mi>i</mi><mo>,</mo><mi>−</mi><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>ϵ</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{-1}(\epsilon_{i,-1}|\epsilon_{i1})</annotation></semantics></math>
is the density function of the other variables, conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ϵ</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\epsilon_{i1}</annotation></semantics></math>.
This can then be rearanged to <span class="math display">$$\begin{align}
\mathcal{L} (\bm{R}_{i}| \bm{\beta}_1, ..., \bm{\beta}_J, \bm{\Sigma},
\bm{X}_i , \bm{P} ) = &amp;  \int f_1(\epsilon_{i1}) \left[
\prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| \theta_{ij}, \bm{P}_{hj}) \right]
d\theta_{i1} \nonumber \\ &amp; \int ...
\int  f_{-1}(\epsilon_{i,-1}|\epsilon_{i1}) \prod_{j=2}^J
\prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| \theta_{ij}, \bm{P}_{hj})
d\theta_{i2} ... d\theta_{iJ} \label{eq:introeq}
\end{align}$$</span> Changing to the numerical quadrature representation
<span class="math display">$$\begin{align}
\mathcal{L} (\bm{R}_{i}| \bm{\beta}_1, ..., \bm{\beta}_J, \bm{\Sigma},
\bm{X}_i , \bm{P} ) = &amp; \sum_{q_1=1}^Q \delta f_1(q_1 - \bm{X}_i
\beta_1) \left[ \prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| q_1, \bm{P}_{hj})
\right]   \nonumber \\ &amp; \sum_{q_2=1}^Q ... \sum_{q_J=1}^Q
\delta^{J-1}  f_{-1}(\epsilon_{i,-1}(q_{-1})|q_{i1}) \prod_{j=2}^J
\prod_{h_j=1}^{H_j} {\rm Pr}(R_{ijh}| q_{ij}, \bm{P}_{hj})
\label{eq:introeq}
\end{align}$$</span></p>
<p>this is additively seperable—the max with respect to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mn>𝟏</mn></msub><annotation encoding="application/x-tex">\bm{\beta_1}</annotation></semantics></math>
can be found only with data for the first construct. The other
constructs can be relabeled and each can be maximized in this way.
Similarly, the variances can be independently found in this way.</p>
<p>The <code>Dire</code> package helps analyists estimate coefficients
in two potentially useful ways. First the parameters in the regression
can be directly estimated, where the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
values are used from the above model fit. Second, <code>Dire</code> can
generate plausible values from the fitted likelihood surface.</p>
<p>The direct estimation method has the advantage of using the latent
space to identify coefficients. However, it can be time consuming to fit
a latent parameter model.</p>
<p>The plausible value approach has two advantages. First, it can be
used to fit a saturated model (with all possible coefficients of
interest) and then other models that use subsets or linear combinations
of those coefficients can be fit to the plausible values. Second, doing
this saves time relative to fitting a direct estimation model per
regression specification.</p>
<p>This document first describes how parameters are estimated in the
latest space. The third section describes the variance estimator. The
subsequent section describes estimation of degrees of freedom. Finally,
it describes plausible value generation. Each section starts by
describing the univariate case and then describes the case with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>
potentially correlated constructs or subscales. Additionally, while the
introduction has not mentioned weights, they are incorporated in the
subsequent sections.</p>
<div class="section level2">
<h2 id="parameter-estimation">Parameter Estimation<a class="anchor" aria-label="anchor" href="#parameter-estimation"></a>
</h2>
<p>Starting with the single construct MML model for test data for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
individuals, conditional on a set of parameters for a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>
test items, the likelihood of a regression equation is <span class="math display">$$\begin{align}
\mathcal{L} (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) =
\prod_{i=1}^N  \left[ \int \frac{1}{\sigma \sqrt{2\pi}} \exp
\frac{-(\theta_i - \bm{X}_i \beta)^2}{2\sigma^2}  \, \prod_{h=1}^H {\rm
Pr}(\bm{R}_{ih}|\theta_i,\bm{P}_h) d\theta_i \right]^{\bm{w}_i}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>
is the likelihood of the regression parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\bm{\beta}</annotation></semantics></math>
with full sample weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_i</annotation></semantics></math>
conditional on item score matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐑</mi><annotation encoding="application/x-tex">\bm{R}</annotation></semantics></math><em>,</em>
student covariate matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\bm{X}</annotation></semantics></math><em>,</em>
and item parameter data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐏</mi><annotation encoding="application/x-tex">\bm{P}</annotation></semantics></math><em>;</em>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
is the variance of the regression residual;
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
student’s latent ability measure that is being integrated out; <span class="math inline">${\rm Pr}(\bm{R}_{ih}|\theta_i, \bm{P}_h)$</span> is
the probability of individual
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>’s
score on test item
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math><em>,</em>
conditional on the student’s ability and item parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐏</mi><mi>h</mi></msub><annotation encoding="application/x-tex">\bm{P}_h</annotation></semantics></math>—see
the appendix for example forms of <span class="math inline">${\rm
Pr}(\bm{R}_{ih}|\theta_i, \bm{P}_h)$</span>.</p>
<p>The integral is evaluated using the trapezoid rule at quadrature
points
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>q</mi></msub><annotation encoding="application/x-tex">t_q</annotation></semantics></math>
and quadrature weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math>
so that <span class="math display">$$\begin{align}
\mathcal{L} (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) &amp;=
\prod_{i=1}^N \left[ \sum_{q=1}^Q \delta \frac{1}{\sigma \sqrt{2\pi}}
\exp \frac{-(t_q - \bm{X}_i \bm{\beta})^2}{2\sigma^2}
\prod_{h=1}^H  {\rm Pr}(\bm{R}_{ih}|t_q, \bm{P}_h)\right]^{\bm{w}_i}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math>
is the distance between any two uniformly spaced quadrature points so
that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mo>=</mo><msub><mi>t</mi><mrow><mi>q</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>t</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\delta = t_{q+1} - t_{q}</annotation></semantics></math>
for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>
that is at least one and less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.
The range and value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
parameterize the quadrature, and its accuracy and should be varied to
ensure convergence. The advantage of the trapezoidal rule is that the
fixed quadrature points allow the values of the probability (the portion
inside the product) to be calculated once per student.</p>
<p>The log-likelihood is given by <span class="math display">$$\begin{align}
\ell (\bm{\beta}, \sigma|\bm{w}, \bm{R}, \bm{X}, \bm{P}) &amp;=
\sum_{i=1}^N \bm{w}_i \, {\rm log} \left[\delta \sum_{q=1}^Q
\frac{1}{\sigma \sqrt{2\pi}} \exp \frac{-(t_q - \bm{X}_i
\bm{\beta})^2}{2\sigma^2} \prod_{j=1}^K  {\rm Pr}(\bm{R}_{ij}|t_q,
\bm{P}_j) \right]
\end{align}$$</span> Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math>
can be removed for optimization, and its presence adds <span class="math inline">${\rm log}(\delta) \sum \bm{w}_i$</span> to the
log-likelihood.</p>
<div class="section level3">
<h3 id="composite-score-estimation">Composite Score Estimation<a class="anchor" aria-label="anchor" href="#composite-score-estimation"></a>
</h3>
<p>When the outcome of interest is composite scores, the parameters are
estimated by separately estimating the coefficients for each subscale
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{\beta}_j</annotation></semantics></math>
for subscale
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>)
and then calculating the composite scores
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\bm{\beta}_c</annotation></semantics></math>)
using subscale weights
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ω</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\omega_j</annotation></semantics></math>).
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝛃</mi><mi>c</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>ω</mi><mi>j</mi></msub><msub><mi>𝛃</mi><mi>j</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{\beta}_{c} &amp;= \sum_{j=1}^J \omega_j \bm{\beta}_j \label{eq:composite}
\end{align}</annotation></semantics></math></p>
<p>The full covariance matrix for the residuals
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛜</mi><annotation encoding="application/x-tex">\bm{\epsilon}</annotation></semantics></math>
vector) is then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>𝚺</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>12</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>1</mn><mi>J</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>12</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mn>2</mn><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>2</mn><mi>J</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>1</mn><mi>J</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>2</mn><mi>J</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mi>J</mi><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{\Sigma} = \left[ \begin{array}{cccc} \sigma_1^2 &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1J} \\ 
                                       \sigma_{12} &amp; \sigma_{2}^2 &amp; \cdots &amp; \sigma_{2J} \\
                                       \vdots      &amp; \vdots       &amp; \ddots &amp; \vdots \\
                                       \sigma_{1J} &amp; \sigma_{2J}  &amp;  \cdots &amp; \sigma_{J}^2 \end{array} \right]
\end{align}</annotation></semantics></math></p>
<p>The likelihood is then <span class="math display">$$\begin{align}
\ell \left( \sigma_{jj^\prime} \left| \bm{\beta}_j,
\bm{\beta}_{j^\prime} , \sigma_j, \sigma_{j^\prime}; \bm{w}, \bm{R},
\bm{X}, \bm{P}\right. \right) &amp;=
\sum_{n=1}^N \bm{w}_n \, {\rm log} \left\{ \int \int \frac{1}{2\pi
\sqrt{\left |{\bm{\Sigma}}_{(jj^\prime)} \right |}}\exp \left(
\hat{\bm{\epsilon}}^T_{jj^\prime} {\bm{\Sigma}}_{(jj^\prime)}^{-1}
\hat{\bm{\epsilon}}_{jj^\prime} \right) \right. \\
&amp;\mathrel{\phantom{=}} \left. \times \left[ \prod_{h=1}^{H_j}  {\rm
Pr}(\bm{R}_{njh}|\theta_j, \bm{P}_{h^\prime}) \right] \left[
\prod_{h^\prime=1}^{H_{j^\prime}} {\rm Pr}(\bm{R}_{nj^\prime
h^\prime}|\theta_{j^\prime}, \bm{P}_{h^\prime}) \right] \right\}
d\theta_j d\theta_{j^\prime}
\label{eq:compcovlnl}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>𝚺</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\bm{\Sigma}_{(jj^\prime)}|</annotation></semantics></math>
is the determinant of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></msub><annotation encoding="application/x-tex">\bm{\Sigma}_{(jj^\prime)}</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mover><mi>𝚺</mi><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></msub><mo>≡</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mi>j</mi><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><msup><mi>j</mi><mi>′</mi></msup><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\tilde{\bm{\Sigma}}_{(jj^\prime)} \equiv  \left[ \begin{array}{cc} \sigma_j^2 &amp; \sigma_{jj^\prime}  \\ 
                                                     \sigma_{jj^\prime} &amp; \sigma_{j^\prime}^2 \end{array} \right]
\end{align}</annotation></semantics></math> and the residual term is
defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mover><mi>𝛜</mi><mo accent="true">̂</mo></mover><mrow><mi>j</mi><msup><mi>j</mi><mi>′</mi></msup></mrow></msub><mo>≡</mo></mtd><mtd columnalign="left" style="text-align: left"><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><msub><mi>𝐗</mi><mi>i</mi></msub><msub><mi>𝛃</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>θ</mi><msup><mi>j</mi><mi>′</mi></msup></msub><mo>−</mo><msub><mi>𝐗</mi><mi>i</mi></msub><msub><mi>𝛃</mi><msup><mi>j</mi><mi>′</mi></msup></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\hat{\bm{\epsilon}}_{jj^\prime} \equiv &amp; \left( \begin{array}{c} \theta_j - \bm{X}_i \bm{\beta}_j  \\ \theta_{j^\prime} - \bm{X}_i \bm{\beta}_{j^\prime} \end{array} \right) 
\end{align}</annotation></semantics></math> Notice that the parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{\beta}_j</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛃</mi><msup><mi>j</mi><mi>′</mi></msup></msub><annotation encoding="application/x-tex">\bm{\beta}_{j^\prime}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><mi>j</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma_j^2</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>σ</mi><msup><mi>j</mi><mi>′</mi></msup><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma^2_{j^\prime}</annotation></semantics></math>
are taken from the subscale estimation, so the only parameter not fixed
is the covariance term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>σ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\sigma_{ij}</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="variance-estimation">Variance Estimation<a class="anchor" aria-label="anchor" href="#variance-estimation"></a>
</h2>
<p>Starting with the univariate case, estimating variance of the
parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\bm{\beta}</annotation></semantics></math>
can be done in one of several ways.</p>
<p>The inverse Hessian matrix is a consistent estimator when the
estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\bm{\beta}</annotation></semantics></math>
is consistent (Green, 2003, p. 520): <span class="math display">$$\begin{align}
{\rm Var}(\bm{\beta}) = -\bm{H}(\bm{\beta})^{-1} =  -
\left[\frac{\partial^2 \ell(\bm{\beta}, \sigma|\bm{w}, \bm{R},
\bm{X})}{\partial \bm{\beta}^2} \right]^{-1} \label{eq:vbeta}
\end{align}$$</span> This variance is returned when the variance method
is set to <code>consistent</code> or left as the default.</p>
<p>A class of variance estimators typically called “sandwich” or
“robust” variance estimators allow for variation in the residual and are
of the form <span class="math display">$$\begin{align}
{\rm Var}(\bm{\beta}) = H(\bm{\beta})^{-1} \bm{V} H(\bm{\beta})^{-1}
\label{eq:sandwich}
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>
is an estimate of the variance of the summed score function (Binder,
1983).</p>
<p>For a convenience sample, we provide two robust estimators. First,
the so-called <code>robust</code> (Huber or Huber-White) variance
estimator uses
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐕</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mi>′</mi></msup></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{V} &amp;= \sum_{i=1}^N \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta} \right] \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta} \right]^{'} \label{eq:scorebased}
\end{align}</annotation></semantics></math></p>
<p>Second, for the <code>cluster robust</code> case, the partial
derivatives are summed within the cluster so that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐕</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>n</mi><mi>′</mi></msup></munderover><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mi>′</mi></msup></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{V} &amp;= \sum_{c=1}^{n^\prime} \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} \right] \left[ \frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} \right]^{'}
\end{align}</annotation></semantics></math> where there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>n</mi><mi>′</mi></msup><annotation encoding="application/x-tex">n^\prime</annotation></semantics></math>
clusters, indexed by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>,
and the partial derivatives are summed within the group of which there
are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>c</mi></msub><annotation encoding="application/x-tex">n_c</annotation></semantics></math>
members:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>c</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>c</mi></msub></munderover><mfrac><mrow><mi>∂</mi><mo>ℓ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>,</mo><mi>σ</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐰</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐑</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>∂</mi><mi>β</mi></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\frac{\partial \ell(\beta, \sigma|\bm{w}_c, \bm{R}_c, \bm{X}_c)}{\partial \beta} &amp;= \sum_{i=1}^{n_c} \frac{\partial \ell(\beta, \sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta}
\end{align}</annotation></semantics></math></p>
<p>Finally, <code>Dire</code> impelments the survey sampling method
called the <code>Taylor series</code> method and uses the same formula
as Eq. , but
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\bm{V}</annotation></semantics></math>
is the estimate of the variance of the score vector (Binder, 1983). Our
implementation assumes a two-stage design with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>a</mi></msub><annotation encoding="application/x-tex">n_a</annotation></semantics></math>
primary sampling units (PSUs) in stratum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
and summed across the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
strata according to
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>𝐕</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>A</mi></munderover><msub><mi>𝐕</mi><mi>a</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{V} &amp;= \sum_{a=1}^A \bm{V}_a
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐕</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\bm{V}_a</annotation></semantics></math>
is a variance estimate for stratum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
and is defined by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐕</mi><mi>a</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><msub><mi>n</mi><mi>a</mi></msub><mrow><msub><mi>n</mi><mi>a</mi></msub><mo>−</mo><mn>1</mn></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>a</mi></msub></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐬</mi><mi>p</mi></msub><mo>−</mo><msub><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐬</mi><mi>p</mi></msub><mo>−</mo><msub><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{V}_a &amp;= \frac{n_a}{n_a -1} \sum_{p=1}^{n_a} \left( \bm{s}_p - \bar{\bm{s}}_a \right)\left( \bm{s}_p - \bar{\bm{s}}_a \right)' \label{eq:Va}
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>p</mi></msub><annotation encoding="application/x-tex">s_p</annotation></semantics></math>
is the sum of the weighted (or pseudo-) score vector that includes all
units in PSU
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
in stratum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover><mi>a</mi></msub><annotation encoding="application/x-tex">\bar{\bm{s}}_a</annotation></semantics></math>
is the (unweighted) mean of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐬</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\bm{s}_p</annotation></semantics></math>
terms in stratum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
so that <span class="math display">$$\begin{align}
s_p &amp;=\sum_{i \in {\rm PSU} \ p}\frac{\partial \ell(\beta,
\sigma|\bm{w}_i, \bm{R}_i, \bm{X}_i)}{\partial \beta}    &amp;
\bar{\bm{s}}_a&amp;= \frac{1}{n_a} \sum_{p \in {\rm stratum} \ a} s_p
\end{align}$$</span></p>
<p>When a stratum has only one PSU,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐕</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\bm{V}_a</annotation></semantics></math>
is undefined. The best approach is for the analyst to adjust the strata
and PSU identifiers, in a manner consistent with the sampling approach,
to avoid singleton strata. Two simpler, automated, but less defensible
options are available in <code>Dire</code>. First, the strata with
single PSUs can be dropped from the variance estimation, yielding an
underestimate of the variance.</p>
<p>The second option is for the singleton stratum to use the overall
mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>p</mi></msub><annotation encoding="application/x-tex">s_p</annotation></semantics></math>
in place of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>s</mi><mo accent="true">‾</mo></mover><mi>a</mi></msub><annotation encoding="application/x-tex">\bar{s}_a</annotation></semantics></math>.
So,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><msup><mi>n</mi><mi>′</mi></msup></mfrac><mo>∑</mo><msub><mi>s</mi><mi>p</mi></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bar{\bm{s}} &amp;= \frac{1}{n^\prime} \sum s_p
\end{align}</annotation></semantics></math> where the sum is across all
PSUs, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>n</mi><mi>′</mi></msup><annotation encoding="application/x-tex">n^\prime</annotation></semantics></math>
is the number of PSUs across all strata. Then, for each singleton
stratum, Eq.  becomes
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐕</mi><mi>a</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐬</mi><mi>p</mi></msub><mo>−</mo><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐬</mi><mi>p</mi></msub><mo>−</mo><mover><mi>𝐬</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{V}_a &amp;= 2 \left( \bm{s}_p - \bar{\bm{s}} \right)\left( \bm{s}_p - \bar{\bm{s}} \right)' \label{eq:Va1}
\end{align}</annotation></semantics></math> where the value 2 is used in
place of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><msub><mi>n</mi><mi>a</mi></msub><mrow><msub><mi>n</mi><mi>a</mi></msub><mo>−</mo><mn>1</mn></mrow></mfrac><annotation encoding="application/x-tex">\frac{n_a}{n_a-1}</annotation></semantics></math>,
which is undefined when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>a</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_a=1</annotation></semantics></math>.
This option can underestimate the variance but is thought to more likely
overestimate it.</p>
<p>While not advisable, it is possible to assume information equality
where the hessian (eq. ) and the score vector based covariance (eq. )
are equal. When a user sets <code>gradientHessian=TRUE</code> the
<code>Dire</code> package uses this short hand in calculating any of the
above results. Using this option is necessary to get agreement with the
AM software on variance terms.</p>
<div class="section level3">
<h3 id="univariate-degrees-of-freedom">Univariate degrees of freedom<a class="anchor" aria-label="anchor" href="#univariate-degrees-of-freedom"></a>
</h3>
<p>For the Taylor series estimator, the degrees of freedom estimator
also uses the Welch-Satterthwaite (WS) degrees of freedom estimate
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>o</mi><msub><mi>f</mi><mrow><mi>W</mi><mi>S</mi></mrow></msub></mrow><annotation encoding="application/x-tex">dof_{WS}</annotation></semantics></math>).
The WS weights require an estimate of the degrees of variance per
independent group. For a clustered sample, that is available at the
stratum.</p>
<p>Following Binder (1983) and Cohen (2002), the contribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>c</mi><mi>s</mi></msub><annotation encoding="application/x-tex">c_s</annotation></semantics></math>
to the degrees of freedom from stratum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐳</mi><mrow><mi>u</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\bm{z}_{uj}</annotation></semantics></math>
from the section, “Estimation of Standard Errors of Weighted Means When
Plausible Values Are Not Present, Using the Taylor Series Method,”
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>c</mi><mi>s</mi></msub><mo>=</mo><msub><mi>w</mi><mi>s</mi></msub><mfrac><msub><mi>n</mi><mi>s</mi></msub><mrow><msub><mi>n</mi><mi>s</mi></msub><mo>−</mo><mn>1</mn></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>s</mi></msub></munderover><msub><mi>𝐳</mi><mrow><mi>u</mi><mi>j</mi></mrow></msub><msubsup><mi>𝐳</mi><mrow><mi>u</mi><mi>j</mi></mrow><mi>T</mi></msubsup></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
c_s = w_s \frac{n_s}{n_s-1} \sum_{u=1}^{n_s} \bm{z}_{uj} \bm{z}_{uj}^T
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>
indexes the PSUs in the stratum, of which there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>s</mi></msub><annotation encoding="application/x-tex">n_s</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>s</mi></msub><annotation encoding="application/x-tex">w_s</annotation></semantics></math>
is the stratum weight, or the sum, in that stratum, of all the unit’s
full sample weights. Using the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>c</mi><mi>s</mi></msub><annotation encoding="application/x-tex">c_s</annotation></semantics></math>
values, the degrees of freedom is <span class="math display">$$\begin{align}
{\rm dof}_{\rm WS} = \frac{\left(\sum c_s \right)^2}{\sum c_s^2}
\end{align}$$</span> which uses the formula of Satterthwaite (1946),
assuming one degree of freedom per stratum.</p>
</div>
<div class="section level3">
<h3 id="composite-score-variances">Composite Score Variances<a class="anchor" aria-label="anchor" href="#composite-score-variances"></a>
</h3>
<p>In the case of composite scores the variance becomes more complex and
only one method is supported, Taylor series.</p>
<p>The log-likelihood of composite scores is additively separable, the
covariances (including the variances) can be calculated in two steps
using Eq. . First, the covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛏</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation></semantics></math>
is formed, and then the composite covariance terms are estimated as the
variance of a linear combination of the elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛏</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation></semantics></math>.</p>
<p>In the first step, any of the methods in the section “Variance
Estimation” are applied to Eq. , treating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛏</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation></semantics></math>
in the same fashion Eq.  treats
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛃</mi><annotation encoding="application/x-tex">\bm{\beta}</annotation></semantics></math>.
This step results in a block diagonal inverse Hessian matrix, with a
block for each subscale, and a potentially dense matrix for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\bm{V}</annotation></semantics></math>.
Each matrix is square and has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>ζ</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S \cdot (\zeta+1)</annotation></semantics></math>
rows and columns, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ζ</mi><annotation encoding="application/x-tex">\zeta</annotation></semantics></math>
is the number of elements in the regression formula (each subscale), to
which one is added for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
terms.</p>
<p>This step results in the following matrix: <span class="math display">$$\begin{align}
{\rm Var}(\bm{\xi})=H(\bm{\xi})^{-1} \bm{V} H(\bm{\xi})^{-1}
\end{align}$$</span></p>
<p>For the second step, the composite coefficient then has an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
variance term of <span class="math display">$$\begin{align}
{\rm Var}(\bm{\xi}_{ci}) &amp;= \bm{e}_i H(\bm{\xi})^{-1} \bm{V}
H(\bm{\xi})^{-1} \bm{e}_i
\end{align}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛏</mi><mrow><mi>c</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\bm{\xi}_{ci}</annotation></semantics></math>
is the composite coefficient for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
coefficient, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{e}_{i}</annotation></semantics></math>
is the vector of weights arranged such that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝛏</mi><mrow><mi>c</mi><mi>i</mi></mrow></msub><mo>=</mo><msubsup><mi>𝐞</mi><mi>i</mi><mi>T</mi></msubsup><mi>𝛏</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{\xi}_{ci} = \bm{e}_{i}^T \bm{\xi}
\end{align}</annotation></semantics></math> The covariance between two
terms,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math><em>,</em>
is a simple extension <span class="math display">$$\begin{align}
{\rm Cov}(\bm{\beta}_{ci}, \bm{\beta}_{cj}) &amp;= \bm{e}_i
H(\bm{\beta})^{-1} \bm{V} H(\bm{\beta})^{-1} \bm{e}_j
\end{align}$$</span> which uses the definition,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝛏</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mi>𝐞</mi><mi>j</mi><mi>T</mi></msubsup><mi>𝛏</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{\xi}_{cj} &amp;= \bm{e}_{j}^T \bm{\xi}
\end{align}</annotation></semantics></math></p>
<p>A simple example may help clarify. Imagine a composite score composed
of two subscales, 1 and 2, with weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mn>1</mn></msub><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">\omega_1 = 0.4</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mn>2</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">\omega_2=0.6</annotation></semantics></math>.
Supposed a user is interested in a regression of the form
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>θ</mi><mo>=</mo><mi>a</mi><mo>+</mo><msub><mi>x</mi><mn>1</mn></msub><mo>⋅</mo><mi>b</mi><mo>+</mo><mi>ϵ</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\theta = a + x_1 \cdot b + \epsilon \label{eq:compex} \\
\epsilon \sim N(0,\sigma)
\end{align}</annotation></semantics></math> Then the regression in Eq. 
would be fit once for subscale 1 and once for subscale 2; the first fit
would yield estimated values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>σ</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\left\{ a_1, b_1, \sigma_1 \right\}</annotation></semantics></math>,
and the second fit would yield
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><msub><mi>σ</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\left\{ a_2, b_2, \sigma_2 \right\}</annotation></semantics></math>.
The estimated value, for example,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>c</mi></msub><annotation encoding="application/x-tex">a_c</annotation></semantics></math>,
would be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>c</mi></msub><mo>=</mo><mn>0.4</mn><mo>⋅</mo><msub><mi>α</mi><mn>1</mn></msub><mo>+</mo><mn>0.6</mn><mo>⋅</mo><msub><mi>α</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">a_c = 0.4\cdot\alpha_1 + 0.6\cdot\alpha_2</annotation></semantics></math>.
By stacking the estimates together,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>𝛉</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>b</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>b</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{\theta} &amp;= \begin{bmatrix}
 a_1 \\
 b_1 \\
 \sigma_1\\
 a_2 \\
 b_2\\
 \sigma_2
\end{bmatrix}
\end{align}</annotation></semantics></math> the covariance matrix can
then be estimated and will result in a matrix <span class="math inline">$\bm{\Omega} \equiv  {\rm Var}(\bm{\beta})$</span>
from Eq.  that has six rows and six columns. Using the vector
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝐞</mi><mn>1</mn></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\bm{e}_1 &amp;= \begin{bmatrix}
 0.4 \\
 0 \\
 0\\
 0.6 \\
 0\\
 0
\end{bmatrix}
\end{align}</annotation></semantics></math> it can easily be confirmed
that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>c</mi></msub><mo>=</mo><msubsup><mi>𝐞</mi><mn>1</mn><mi>T</mi></msubsup><mi>𝛏</mi></mrow><annotation encoding="application/x-tex">a_c = \bm{e}_1^T \bm{\xi}</annotation></semantics></math>,
so <span class="math inline">${\rm Var}(a_c)= \bm{e}_1^T \bm{\Omega}
\bm{e}_1$</span>.</p>
</div>
<div class="section level3">
<h3 id="composite-degrees-of-freedom">Composite degrees of freedom<a class="anchor" aria-label="anchor" href="#composite-degrees-of-freedom"></a>
</h3>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Binder, D. A. (1983). On the variances of asymptotically normal
estimators from complex surveys. <em>International Statistical
Review</em>, <em>51</em>(3), 279–292.</p>
<p>Black, P. E. (2019). Big-O notation. In P. E. Black (Ed.),
<em>Dictionary of algorithms and data structures</em>. Washington, DC:
National Institute of Standards and Technology. Retrieved from <a href="https://www.nist.gov/dads/HTML/bigOnotation.html" class="external-link uri">https://www.nist.gov/dads/HTML/bigOnotation.html</a></p>
<p>Cohen, J. D., &amp; Jiang, T. (1999). Comparison of partially
measured latent traits across nominal subgroups. <em>Journal of the
American Statistical Association</em>, 94(<em>448</em>), 1035–1044.</p>
<p>Green, W. H. (2003). <em>Econometric analysis</em> Upper Saddle
River, NJ: Prentice Hall.</p>
<p>Huber, P. J. (1967). The behavior of maximum likelihood estimates
under nonstandard conditions. <em>Proceedings of the Fifth Berkeley
Symposium of Mathematical Statistics and Probability</em>, Vol. I:
<em>Statistics</em> (pp. 221–233). Berkeley, CA: University of
California Press.</p>
<p>Johnson, S. G. (2010). <em>Notes on the convergence of
trapezoidal-rule quadrature</em>. Retrieved from <a href="https://math.mit.edu/~stevenj/trapezoidal.pdf" class="external-link uri">https://math.mit.edu/~stevenj/trapezoidal.pdf</a></p>
<p>McCullagh, P. &amp; Nelder, J. A. (1989). <em>Generalized linear
models</em>. (2nd ed.). London, UK: Chapman &amp; Hall/CRC.</p>
<p>NAEP. (2008). The generalized partial credit model [NAEP Technical
Documentation Website]. Retrieved from <a href="https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_gen.aspx" class="external-link uri">https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_gen.aspx</a>.</p>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix
estimator and a direct test for heteroskedasticity.
<em>Econometrica</em>, 48(<em>4</em>), 817–838.</p>
</div>
<div class="section level2">
<h2 id="appendix--test-probability-density-functions">Appendix. Test Probability Density Functions<a class="anchor" aria-label="anchor" href="#appendix--test-probability-density-functions"></a>
</h2>
<p>For all cases scored as either correct or incorrect, we use the
<em>three parameter logit</em> (3PL) model:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>g</mi><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msub><mi>g</mi><mi>j</mi></msub></mrow><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>D</mi><mspace width="0.167em"></mspace><msub><mi>a</mi><mi>j</mi></msub><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= g_j + \frac{1-g_j}{1+\exp\left[ -D \, a_j \, (\theta_i - d_j)\right]} \label{eq:3PL}
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>g</mi><mi>j</mi></msub><annotation encoding="application/x-tex">g_j</annotation></semantics></math>
is the guessing parameter,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>j</mi></msub><annotation encoding="application/x-tex">a_j</annotation></semantics></math>
is the discrimination factor,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding="application/x-tex">d_j</annotation></semantics></math>
is the item difficulty, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is a constant, usually set to 1.7, to map the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding="application/x-tex">d_j</annotation></semantics></math>
terms to a probit-like space; this term is applied by tradition.</p>
<p>When a <em>two parameter logit</em> (2PL) is used, Eq.  is modified
to omit
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>g</mi><mi>j</mi></msub><annotation encoding="application/x-tex">g_j</annotation></semantics></math>
(effectively setting it to zero):
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>D</mi><mspace width="0.167em"></mspace><msub><mi>a</mi><mi>j</mi></msub><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[ -D \, a_j \, (\theta_i - d_j)\right]} \label{eq:2PL}
\end{align}</annotation></semantics></math></p>
<p>When a <em>Rasch model</em> is used, Eq.  is further modified to set
all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>j</mi></msub><annotation encoding="application/x-tex">a_j</annotation></semantics></math>
to a single
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math><em>,</em>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is set to one.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>a</mi><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[ - a \, (\theta_i - d_j)\right]} \label{eq:Rasch}
\end{align}</annotation></semantics></math></p>
<p>The <em>Graded Response Model</em> (GRM) has a probability density
that generalizes an ordered logit (McCullagh &amp; Nelder, 1989):
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>D</mi><mspace width="0.167em"></mspace><msub><mi>a</mi><mi>j</mi></msub><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac><mo>−</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mi>D</mi><mspace width="0.167em"></mspace><msub><mi>a</mi><mi>j</mi></msub><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><mn>1</mn><mo>+</mo><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{1}{1+\exp\left[-D\, a_j \, (\theta_i - d_{R_{ij},j})\right]} - \frac{1}{1+\exp\left[-D\, a_j \, (\theta_i - d_{1+R_{ij},j})\right]} \label{eq:grm}
\end{align}</annotation></semantics></math> Here the parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐏</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{P}_j</annotation></semantics></math>
are the cut points
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">d_{cj}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mn>0</mn><mi>j</mi></mrow></msub><mo>=</mo><mi>−</mi><mi>∞</mi></mrow><annotation encoding="application/x-tex">d_{0j}=-\infty</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>C</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">d_{C+1,j}=\infty</annotation></semantics></math>.
In the first term on the right side of Eq. , the subscript
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">R_{ij}</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mrow><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">d_{R_{ij},j}</annotation></semantics></math>
indicates it is the cut point associated with the response level to item
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
for person
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
whereas the last subscript
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>)
indicates that it is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
term for item
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>.
In the second term, the cut point above that cut point is used.</p>
<p>The <em>Generalized Partial Credit Model</em> (GPCM) has a
probability density that generalizes a multinomial logit (McCullagh
&amp; Nelder, 1989)
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></munderover><mi>D</mi><msub><mi>a</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mi>C</mi></munderover><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><mi>r</mi></munderover><mi>D</mi><msub><mi>a</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{\exp \left[ \sum_{c=0}^{R_{ij}} D a_j (\theta_i - d_{cj}) \right]}{\sum_{r=0}^{C} \exp \left[ \sum_{c=0}^{r} D a_j (\theta_i - d_{cj}) \right]}
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
indexes cut points, of which there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math><em>,</em>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
indexes the item.</p>
<p>The GPCM equation has an indeterminancy because all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding="application/x-tex">d_j</annotation></semantics></math>
terms could increase and make the values of the probability the same. We
can solve the indeterminacy in several ways.</p>
<p>NAEP (2008) uses a mean difficulty
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mi>j</mi></msub><annotation encoding="application/x-tex">b_j</annotation></semantics></math>),
and the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding="application/x-tex">d_j</annotation></semantics></math>
values are then given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>d</mi><mrow><mn>0</mn><mi>j</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>0</mn></mtd><mtd columnalign="right" style="text-align: right"><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>b</mi><mi>j</mi></msub><mo>−</mo><msub><mi>δ</mi><mrow><mi>j</mi><mi>c</mi></mrow></msub><mspace width="0.167em"></mspace><mo>;</mo><mspace width="0.167em"></mspace><mn>1</mn><mo>≤</mo><mi>c</mi><mo>≤</mo><mi>C</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
d_{0j} &amp;= 0 &amp;       d_{cj} &amp;= b_j - \delta_{jc} \, ; \, 1 \leq c \leq C
\end{align}</annotation></semantics></math> where the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>δ</mi><mrow><mi>j</mi><mi>c</mi></mrow></msub><annotation encoding="application/x-tex">\delta_{jc}</annotation></semantics></math>
values are estimated so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><msub><mi>δ</mi><mrow><mi>j</mi><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">0=\sum_{c=1}^{C} \delta_{jc}</annotation></semantics></math>.
In this package, when the <code>polyParamTab</code> has an
<code>itemLocation</code>, it serves as <code>b</code>. When there is no
<code>itemLocation</code>, the package uses the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math>
values directly
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>d</mi><mrow><mn>0</mn><mi>j</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>0</mn></mtd><mtd columnalign="right" style="text-align: right"><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>δ</mi><mrow><mi>j</mi><mi>c</mi></mrow></msub><mspace width="0.167em"></mspace><mo>;</mo><mspace width="0.167em"></mspace><mn>1</mn><mo>≤</mo><mi>c</mi><mo>≤</mo><mi>C</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
d_{0j} &amp;= 0 &amp;       d_{cj} &amp;= \delta_{jc} \, ; \, 1 \leq c \leq C
\end{align}</annotation></semantics></math></p>
<p>When a <em>Partial Credit Model</em> (PCM) is used, and the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is set to one, whereas
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mi>j</mi></msub><annotation encoding="application/x-tex">a_j</annotation></semantics></math>
is again shared across all items. So
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐑</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐏</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>R</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></munderover><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mi>C</mi></munderover><mo>exp</mo><mrow><mo stretchy="true" form="prefix">[</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><mi>r</mi></munderover><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>d</mi><mrow><mi>c</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></mfrac></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\Pr(\bm{R}_{ij}|\theta_i, \bm{P}_j) &amp;= \frac{\exp \left[ \sum_{c=0}^{R_{ij}} a (\theta_i - d_{cj}) \right]}{\sum_{r=0}^{C} \exp \left[ \sum_{c=0}^{r} a (\theta_i - d_{cj}) \right]}
\end{align}</annotation></semantics></math></p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Paul Bailey, Eric Buehler, Sun-joo Lee, Harold Doran, Blue Webb.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
